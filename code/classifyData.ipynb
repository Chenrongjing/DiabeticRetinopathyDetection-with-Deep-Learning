{
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "name": "",
  "signature": "sha256:84e0e46bd1e0e827260104c61c9f89f33c35b5957c88502481b7a215b198fe85"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's first load the training data, after processing it into a single file by executing processImages.py .\n",
      "We use load() for loading the processed file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "import numpy as np\n",
      "from pandas.io.parsers import read_csv\n",
      "from sklearn.utils import shuffle\n",
      "FTRAIN = 'home/ubuntu/data/train_images_128_128_all_new.txt'\n",
      "FTEST = \"/home/ubuntu/data/test_images_128_128_all_new.txt\"\n",
      "\"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
      "\"\"\"\n",
      "os.chdir('../')\n",
      "def load(test=False):\n",
      "    fname = FTEST if test else FTRAIN\n",
      "    with open(fname, 'r') as f: \n",
      "        y=[]\n",
      "        X=[]\n",
      "        for row in csv.reader(f): #each row in the input file is added to X and y lists\n",
      "            r = np.array(map(np.float32, row))\n",
      "            y_ = r[0] #the first column is the label\n",
      "            X_ = np.array(r[range(1,r.shape[0])]).reshape( 1,128,128) #the rest of the columns are images\n",
      "            X.append(X_)\n",
      "            y.append(y_)\n",
      "        #converting X and y to np.arrays\n",
      "        y=np.array(y,dtype=np.uint8)\n",
      "        X=np.array(X) \n",
      "        if test: \n",
      "            y=None #in test files label is irrelevant\n",
      "        else: X, y = shuffle(X, y, random_state=42) #shuffle train records \n",
      "    return X, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After defining the load() method, we actually use it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.setrecursionlimit(20000) # since we might go over the stack size limit\n",
      "X, y = load()   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to use on_epoch_finished and on_epoch_finished:\n",
      "1. We will change some parameters(update_learning_rate and update_momentum) to become Theano shared variables so that they could be update on the fly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import theano\n",
      "def float32(k):\n",
      "    return np.cast['float32'](k)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2. Passing a parametrizable class with a __call__ method as our callback. (Its parameters: nn, which is the NeuralNet instance itself, and train_history, which is the same as nn.train_history_)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class AdjustVariable(object):\n",
      "    def __init__(self, name, start=0.03, stop=0.001):\n",
      "        self.name = name\n",
      "        self.start, self.stop = start, stop\n",
      "        self.ls = None\n",
      "    def __call__(self, nn, train_history):\n",
      "        if self.ls is None:\n",
      "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)    \n",
      "        epoch = train_history[-1]['epoch']\n",
      "        new_value = float32(self.ls[epoch - 1])\n",
      "        getattr(nn, self.name).set_value(new_value)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now train aneuralnetwork model based on the train set. We start by defining the first network (net1) as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lasagne import layers\n",
      "import lasagne.nonlinearities\n",
      "from nolearn.lasagne import NeuralNet\n",
      "from nolearn.lasagne import BatchIterator\n",
      "net1 = NeuralNet(\n",
      "    layers=[\n",
      "        ('input', layers.InputLayer),\n",
      "        ('conv1', layers.Conv2DLayer),\n",
      "        ('pool1', layers.MaxPool2DLayer),\n",
      "        ('conv2', layers.Conv2DLayer),\n",
      "        ('pool2', layers.MaxPool2DLayer),\n",
      "        ('conv3', layers.Conv2DLayer),\n",
      "        ('pool3', layers.MaxPool2DLayer),\n",
      "        ('hidden4', layers.DenseLayer),\n",
      "        ('hidden5', layers.DenseLayer),\n",
      "        ('output', layers.DenseLayer),\n",
      "        ],\n",
      "    input_shape=(None, 1, 128, 128),\n",
      "    conv1_num_filters=32, conv1_filter_size=(3, 3), pool1_pool_size=(2, 2),\n",
      "    conv2_num_filters=64, conv2_filter_size=(2, 2), pool2_pool_size=(2, 2),\n",
      "    conv3_num_filters=128, conv3_filter_size=(2, 2), pool3_pool_size=(2, 2),\n",
      "    hidden4_num_units=500, hidden5_num_units=500,\n",
      "    output_num_units=5, output_nonlinearity=lasagne.nonlinearities.softmax ,\n",
      "    update_learning_rate=theano.shared(float32(0.03)),\n",
      "    update_momentum=theano.shared(float32(0.9)),\n",
      "    regression=False,\n",
      "    batch_iterator_train=BatchIterator(batch_size=214),\n",
      "    on_epoch_finished=[\n",
      "        AdjustVariable('update_learning_rate', start=0.03, stop=0.0001),\n",
      "        AdjustVariable('update_momentum', start=0.9, stop=0.999),\n",
      "        ],\n",
      "    max_epochs=300,\n",
      "    verbose=1,\n",
      "    )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's train net1 according to the train data that we loaded:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net1.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output (50 first iterations) looks something like this:\n",
      "  input                 (None, 1, 128, 128)     produces   16384 outputs\n",
      "  conv1                 (None, 32, 126, 126)    produces  508032 outputs\n",
      "  pool1                 (None, 32, 63, 63)      produces  127008 outputs\n",
      "  conv2                 (None, 64, 62, 62)      produces  246016 outputs\n",
      "  pool2                 (None, 64, 31, 31)      produces   61504 outputs\n",
      "  conv3                 (None, 128, 30, 30)     produces  115200 outputs\n",
      "  pool3                 (None, 128, 15, 15)     produces   28800 outputs\n",
      "  hidden4               (None, 500)             produces     500 outputs\n",
      "  hidden5               (None, 500)             produces     500 outputs\n",
      "  output                (None, 5)               produces       5 outputs\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       0.88311       0.86840      1.01694      0.73472  69.54s\n",
      "      2       0.86898       0.86781      1.00135      0.73472  69.52s\n",
      "      3       0.86820       0.86693      1.00146      0.73472  69.53s\n",
      "      4       0.86743       0.86594      1.00171      0.73472  69.52s\n",
      "      5       0.86658       0.86522      1.00157      0.73472  69.53s\n",
      "      6       0.86520       0.86431      1.00103      0.73472  69.53s\n",
      "      7       0.86371       0.86350      1.00024      0.73472  69.54s\n",
      "      8       0.86223       0.86271      0.99944      0.73472  69.54s\n",
      "      9       0.86130       0.86239      0.99874      0.73472  69.54s\n",
      "     10       0.86052       0.86195      0.99834      0.73472  69.54s\n",
      "     11       0.86019       0.86186      0.99807      0.73472  69.56s\n",
      "     12       0.85917       0.86202      0.99669      0.73472  69.57s\n",
      "     13       0.85872       0.86154      0.99673      0.73472  69.56s\n",
      "     14       0.85884       0.86134      0.99710      0.73472  69.55s\n",
      "     15       0.85800       0.86121      0.99627      0.73472  69.55s\n",
      "     16       0.85825       0.86097      0.99684      0.73472  69.56s\n",
      "     17       0.85766       0.86019      0.99706      0.73472  69.55s\n",
      "     18       0.85663       0.86036      0.99566      0.73472  69.54s\n",
      "     19       0.85719       0.86509      0.99087      0.73472  69.55s\n",
      "     20       0.86032       0.85919      1.00131      0.73472  69.55s\n",
      "     21       0.85599       0.85837      0.99722      0.73472  69.59s\n",
      "     22       0.85449       0.85756      0.99642      0.73472  69.57s\n",
      "     23       0.85435       0.86139      0.99183      0.73472  69.57s\n",
      "     24       0.85736       0.85844      0.99874      0.73472  69.56s\n",
      "     25       0.85377       0.85976      0.99303      0.73472  69.56s\n",
      "     26       0.85334       0.85662      0.99617      0.73472  69.56s\n",
      "     27       0.85110       0.85640      0.99381      0.73472  69.56s\n",
      "     28       0.84942       0.85646      0.99178      0.73457  69.55s\n",
      "     29       0.84826       0.85545      0.99160      0.73457  69.56s\n",
      "     30       0.84725       0.85680      0.98886      0.73457  69.57s\n",
      "     31       0.84656       0.85706      0.98776      0.73472  69.55s\n",
      "     32       0.84620       0.85506      0.98964      0.73457  69.55s\n",
      "     33       0.84595       0.85528      0.98908      0.73457  69.54s\n",
      "     34       0.84395       0.85564      0.98634      0.73457  69.55s\n",
      "     35       0.84257       0.85808      0.98192      0.73457  69.56s\n",
      "     36       0.83999       0.85918      0.97766      0.73457  69.56s\n",
      "     37       0.83834       0.86006      0.97475      0.73472  69.56s\n",
      "     38       0.83617       0.86493      0.96676      0.73443  69.56s\n",
      "     39       0.83533       0.85993      0.97140      0.73500  69.56s\n",
      "     40       0.83252       0.86406      0.96351      0.73457  69.57s\n",
      "     41       0.82859       0.86863      0.95391      0.73443  69.56s\n",
      "     42       0.82244       0.87484      0.94011      0.73415  69.57s\n",
      "     43       0.81922       0.87496      0.93630      0.73187  69.59s\n",
      "     44       0.81487       0.88169      0.92421      0.73143  69.58s\n",
      "     45       0.80963       0.88222      0.91772      0.72929  69.58s\n",
      "     46       0.80292       0.88089      0.91149      0.73042  69.58s\n",
      "     47       0.79969       0.89250      0.89602      0.72831  69.59s\n",
      "     48       0.79234       0.92190      0.85946      0.71776  69.58s\n",
      "     49       0.78943       0.89650      0.88057      0.72687  69.58s\n",
      "     50       0.78019       0.90597      0.86117      0.72558  69.58s"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Estimating predicted labels vs. actual labels using several measures:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = net1.predict(X)\n",
      "import sklearn.metrics as metrics   \n",
      "metrics.accuracy_score(y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " 0.9375960826738029"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.confusion_matrix(y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "array([[25346,    86,   318,    33,    27],\n",
      "       [  435,  1966,    34,     2,     6],\n",
      "       [  909,    23,  4343,     8,     9],\n",
      "       [  134,     7,    20,   706,     6],\n",
      "       [  106,     4,    21,     4,   573]])"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.classification_report(y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'                 precision    recall  f1-score   support\\n\\n\n",
      "            0       0.93      1.00      0.96     25810\\n\n",
      "            1       0.99      0.78      0.87      2443\\n\n",
      "            2       0.97      0.79      0.87      5292\\n\n",
      "            3       0.99      0.79      0.88       873\\n\n",
      "            4       0.97      0.81      0.88       708\\n\n",
      "\\navg / total       0.94      0.94      0.94     35126\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since net1 seems to overfit the train data, we will try to add some robustness by defining net2 to which we added DropoutLayer layers between the existing layers and assigned dropout probabilities to each one of them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net2 = NeuralNet(\n",
      "    layers=[\n",
      "        ('input', layers.InputLayer),\n",
      "        ('conv1', layers.Conv2DLayer),\n",
      "        ('pool1', layers.MaxPool2DLayer),\n",
      "        ('dropout1', layers.DropoutLayer),\n",
      "        ('conv2', layers.Conv2DLayer),\n",
      "        ('pool2', layers.MaxPool2DLayer),\n",
      "        ('dropout2', layers.DropoutLayer),\n",
      "        ('conv3', layers.Conv2DLayer),\n",
      "        ('pool3', layers.MaxPool2DLayer),\n",
      "        ('dropout3', layers.DropoutLayer),\n",
      "        ('hidden4', layers.DenseLayer),\n",
      "        ('dropout4', layers.DropoutLayer),\n",
      "        ('hidden5', layers.DenseLayer),\n",
      "        ('output', layers.DenseLayer),\n",
      "        ],\n",
      "    input_shape=(None, 1, 128, 128),\n",
      "    conv1_num_filters=32, conv1_filter_size=(3, 3), pool1_pool_size=(2, 2),\n",
      "    dropout1_p=0.1,\n",
      "    conv2_num_filters=64, conv2_filter_size=(2, 2), pool2_pool_size=(2, 2),\n",
      "    dropout2_p=0.2,\n",
      "    conv3_num_filters=128, conv3_filter_size=(2, 2), pool3_pool_size=(2, 2),\n",
      "    dropout3_p=0.3,\n",
      "    hidden4_num_units=500,\n",
      "    dropout4_p=0.5,\n",
      "    hidden5_num_units=500,\n",
      "    output_num_units=5, output_nonlinearity=lasagne.nonlinearities.softmax,\n",
      "    update_learning_rate=theano.shared(float32(0.03)),\n",
      "    update_momentum=theano.shared(float32(0.9)),\n",
      "    regression=False,\n",
      "    batch_iterator_train=BatchIterator(batch_size=214),\n",
      "    on_epoch_finished=[\n",
      "        AdjustVariable('update_learning_rate', start=0.03, stop=0.0001),\n",
      "        AdjustVariable('update_momentum', start=0.9, stop=0.999),\n",
      "        ],\n",
      "    max_epochs=300,\n",
      "    verbose=1,\n",
      "    )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's train net2 according to our train data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net2.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output (50 first iterations) looks something like this:\n",
      "  input                 (None, 1, 128, 128)     produces   16384 outputs\n",
      "  conv1                 (None, 32, 126, 126)    produces  508032 outputs\n",
      "  pool1                 (None, 32, 63, 63)      produces  127008 outputs\n",
      "  dropout1              (None, 32, 63, 63)      produces  127008 outputs\n",
      "  conv2                 (None, 64, 62, 62)      produces  246016 outputs\n",
      "  pool2                 (None, 64, 31, 31)      produces   61504 outputs\n",
      "  dropout2              (None, 64, 31, 31)      produces   61504 outputs\n",
      "  conv3                 (None, 128, 30, 30)     produces  115200 outputs\n",
      "  pool3                 (None, 128, 15, 15)     produces   28800 outputs\n",
      "  dropout3              (None, 128, 15, 15)     produces   28800 outputs\n",
      "  hidden4               (None, 500)             produces     500 outputs\n",
      "  dropout4              (None, 500)             produces     500 outputs\n",
      "  hidden5               (None, 500)             produces     500 outputs\n",
      "  output                (None, 5)               produces       5 outputs\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       0.89192       0.87145      1.02349      0.73472  71.97s\n",
      "      2       0.86971       0.87088      0.99866      0.73472  71.98s\n",
      "      3       0.86903       0.87051      0.99830      0.73472  71.98s\n",
      "      4       0.86820       0.86975      0.99822      0.73472  71.99s\n",
      "      5       0.86776       0.86988      0.99756      0.73472  72.00s\n",
      "      6       0.86748       0.87025      0.99681      0.73472  72.00s\n",
      "      7       0.86658       0.86974      0.99637      0.73472  72.01s\n",
      "      8       0.86618       0.86885      0.99693      0.73472  72.01s\n",
      "      9       0.86590       0.86840      0.99712      0.73472  72.01s\n",
      "     10       0.86454       0.86783      0.99621      0.73472  72.01s\n",
      "     11       0.86362       0.86714      0.99594      0.73472  72.01s\n",
      "     12       0.86305       0.86674      0.99575      0.73472  72.04s\n",
      "     13       0.86135       0.86605      0.99458      0.73472  72.03s\n",
      "     14       0.86066       0.86564      0.99424      0.73472  72.01s\n",
      "     15       0.86000       0.86648      0.99253      0.73472  72.01s\n",
      "     16       0.85865       0.86531      0.99231      0.73472  72.03s\n",
      "     17       0.85714       0.86438      0.99162      0.73472  72.03s\n",
      "     18       0.85692       0.86431      0.99145      0.73472  72.06s\n",
      "     19       0.85646       0.86248      0.99302      0.73472  72.03s\n",
      "     20       0.85554       0.86352      0.99076      0.73472  72.01s\n",
      "     21       0.85419       0.86173      0.99125      0.73472  72.01s\n",
      "     22       0.85378       0.86262      0.98975      0.73472  72.04s\n",
      "     23       0.85245       0.86235      0.98852      0.73472  72.04s\n",
      "     24       0.85171       0.86142      0.98872      0.73472  72.04s\n",
      "     25       0.85164       0.86119      0.98892      0.73472  72.04s\n",
      "     26       0.85068       0.86145      0.98751      0.73472  72.05s\n",
      "     27       0.84957       0.86076      0.98701      0.73472  72.07s\n",
      "     28       0.84907       0.85933      0.98806      0.73472  72.05s\n",
      "     29       0.84787       0.85971      0.98623      0.73472  72.07s\n",
      "     30       0.84752       0.85985      0.98566      0.73472  72.05s\n",
      "     31       0.84749       0.86010      0.98534      0.73472  72.07s\n",
      "     32       0.84682       0.85936      0.98541      0.73472  72.06s\n",
      "     33       0.84566       0.85912      0.98433      0.73472  72.05s\n",
      "     34       0.84522       0.85989      0.98295      0.73472  72.05s\n",
      "     35       0.84298       0.85947      0.98082      0.73472  72.06s\n",
      "     36       0.84298       0.85912      0.98122      0.73472  72.07s\n",
      "     37       0.84219       0.85987      0.97943      0.73472  72.08s\n",
      "     38       0.84064       0.86012      0.97735      0.73472  72.10s\n",
      "     39       0.83988       0.85870      0.97809      0.73472  72.08s\n",
      "     40       0.83903       0.85920      0.97653      0.73457  72.08s\n",
      "     41       0.83840       0.85993      0.97496      0.73457  72.08s\n",
      "     42       0.83650       0.86096      0.97159      0.73457  72.06s\n",
      "     43       0.83530       0.85891      0.97252      0.73457  72.06s\n",
      "     44       0.83302       0.85950      0.96919      0.73472  72.06s\n",
      "     45       0.83253       0.85761      0.97076      0.73443  72.07s\n",
      "     46       0.82992       0.85778      0.96753      0.73457  72.05s\n",
      "     47       0.82966       0.85850      0.96641      0.73443  72.05s\n",
      "     48       0.82765       0.85866      0.96388      0.73472  72.05s\n",
      "     49       0.82792       0.85839      0.96450      0.73500  72.08s\n",
      "     50       0.82354       0.85845      0.95934      0.73500  72.06s"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Estimating predicted labels vs. actual labels using several measures seems to yield even better results for net2 :)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = net2.predict(X)\n",
      "import sklearn.metrics as metrics   \n",
      "metrics.accuracy_score(y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "0.94075613505665323"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.confusion_matrix(y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "array([[25699,     8,    89,     6,     8],\n",
      "       [  526,  1906,     9,     1,     1],\n",
      "       [ 1101,     5,  4180,     1,     5],\n",
      "       [  158,     0,    21,   689,     5],\n",
      "       [  125,     0,    12,     0,   571]])"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.classification_report(y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "'             precision    recall  f1-score   support\\n\\n\n",
      "            0       0.90      1.00      0.94     25808\\n          \n",
      "            1       0.98      0.66      0.79      2443\\n          \n",
      "            2       0.97      0.68      0.80      5292\\n          \n",
      "            3       0.98      0.72      0.83       873\\n          \n",
      "            4       0.99      0.77      0.86       708\\n\n",
      "\\navg / total       0.92      0.91      0.91     35124\\n'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally the trained model is used to predict labels for the test set (after processing it into a single file by executing processImages.py )\n",
      "Because of its large size, the test file was splitted into 10 files (no line breaks) using the following Unix code, exectuted from a directory that contains the source file:\n",
      "\n",
      "split -n l/10 -a1 -d test_images_128_128_all_new.txt new"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predictTest():\n",
      "    import os\n",
      "    FTEST_PATH = '/home/ubuntu/data/splitted'     \n",
      "    outputpath = '/home/ubuntu/data/net1Predictions.txt'\n",
      "    inputNamesPath = '/home/ubuntu/data/test_images_128_128_names_new.txt'\n",
      "    names = read_csv(os.path.expanduser(inputNamesPath))  # load pandas dataframe\n",
      "    names = np.array(names)\n",
      "    with open(outputpath, \"w\") as outfile:\n",
      "        outfile.write(\"image, level\\n\")    \n",
      "    os.chdir(FTEST_PATH)\n",
      "    j=0\n",
      "    for fname in sorted(os.listdir('.'), key=os.path.getmtime):\n",
      "        FTEST = FTEST_PATH+'/'+fname\n",
      "        X, _ = load(test=True)\n",
      "        y_pred = net2.predict(X)\n",
      "        with open(outputpath, \"a\") as outfile:\n",
      "            for i in range(0,X.shape[0]):\n",
      "                if i+j < names.shape[0]:\n",
      "                    filename = str(names[i+j][0])\n",
      "                    label = str(y_pred[i])\n",
      "                    outfile.write( filename +','+ label+\"\\n\")\n",
      "                    #outfile.write(\"\\n\")\n",
      "        j=j+X.shape[0]\n",
      "    os.chdir('../')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running the defined method yields our results file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.setrecursionlimit(20000)\n",
      "predictTest()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}